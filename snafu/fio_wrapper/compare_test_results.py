# uncomment the next line to enable running script directly from shell
##!/usr/bin/python3
#
# script to compare two test runs using statistics from raw elasticsearch results
# generated by benchmark-wrapper fio
#
# for example, to find uuids to run this program against, do (after adjusting date):
#   python3 snafu/utils/query_result_uuids.py ripsaw-smallfile-results date 2021-09-01

import json
import sys

import numpy
from scipy.stats import ttest_ind, mannwhitneyu
from snafu.utils.fetch_es_test_results import connect_es, result_generator_for_uuid
from os import getenv

KiB_per_MiB = 1 << 10
msec_per_sec = 1000.0

# elapsed time for all pods must be within 5%
elapsed_pctdev_threshold = 5.0

# difference threshold - don't show statistically significant differences unless means are different by this much
mean_difference_pct_threshold = 5.0

# 95% confidence threshold that samples are really different mean
confidence_threshold = 0.95


class FioStatException(Exception):
    pass


index_name = "ripsaw-fio-results"
# used by strptime()
datetime_format = "%Y-%m-%dT%H:%M:%S.%f%z"

if len(sys.argv) < 3:
    print("ERROR: must supply both test uuids")
    sys.exit(1)

uuid1 = sys.argv[1]
uuid2 = sys.argv[2]

print("compare uuid %s to uuid %s" % (uuid1, uuid2))

# assumption: both tests were stored in same elasticsearch server
es = connect_es(es_url=getenv('ES_SERVER'))

def analyze_uuid(uuid_in):

  print('')
  print('for test uuid %s:' % uuid_in)
  optype_dict = {}
  max_sample = 0
  pods_per_run = None
  numjobs = None
  iosize_count = 0

  hit_generator = result_generator_for_uuid(es, index_name, uuid_in)

  for hit in hit_generator:
    src = hit["_source"]
    uuid_found = src["uuid"]

    #print(json.dumps(src, indent=2))

    # skip All Clients record since we want to add them up ourselves

    jobname = src['fio']['jobname']
    if jobname == 'All clients':
        continue

    # extract fields from document

    numjobs_str = src['global_options']['numjobs']
    if numjobs is None:
        numjobs = int(numjobs_str)
    elif int(numjobs_str) != numjobs:
        raise FioStatException('numjobs in this document does not match others: %s' % str(src))

    sample = int(src["sample"])
    if max_sample < sample:
        max_sample = sample

    optype = src['fio']['job options']['rw']
    if optype != "randread" and optype != "randwrite" and optype != "read" and optype != "write":
        raise FioStatException('unrecognized operation type for document: %s' % str(src))

    # remove "KiB" from end of bs string
    io_size_KiB = int(src['global_options']['bs'][:-3])

    if optype == "randread":
        result_thru = src['fio']['read']
    else:
        result_thru = src['fio']['write']
    iops = float(result_thru['iops'])
    MiB_per_sec = float(result_thru['bw']) / KiB_per_MiB

    elapsed = float(src['fio']['job_runtime']) / msec_per_sec

    # host is really pod name
    host = src['fio']['hostname']

    if pods_per_run == None:
        pods_per_run = len(src['hosts'])

    # build up a tree optype -> iosize -> sample -> pod
    # so we can compute stats

    # optype layer
    try:
        optype_iosizes = optype_dict[optype]
        if iosize_count < len(optype_iosizes):
            iosize_count = len(optype_iosizes)
    except KeyError:
        optype_iosizes = {}
        optype_dict[optype] = optype_iosizes

    # iosize layer
    try:
        iosize_samples = optype_iosizes[io_size_KiB]
    except KeyError:
        iosize_samples = {}
        optype_iosizes[io_size_KiB] = iosize_samples

    # sample layer
    try:
        sample_pods = iosize_samples[sample]
    except KeyError:
        sample_pods = {}
        iosize_samples[sample] = sample_pods

    # pod layer
    tupl = (elapsed, MiB_per_sec, iops)
    try:
        no_tupl = sample_pods[host]
        raise FioStatException("for optype %s and sample %d, pod IP %s collides with previous pod document" % 
            (optype, sample, host))
    except KeyError:
        # expected outcome
        sample_pods[host] = tupl

  # now validate data

  #print("optypes in test: %s" % str(optype_dict.keys()))
  #print("")

  print(
    "   optype, io-size-KiB, sample, pod,                            IOPS, MiB/s, elapsed, %dev"
  )
  for optype in sorted(optype_dict.keys()):
    optype_iosizes = optype_dict[optype]
    iosizes_keys = optype_iosizes.keys()
    if len(iosizes_keys) < iosize_count:
        print(
            "WARNING: only %d io sizes for optype %s"
            % (len(iosizes_keys), optype)
        )
    for iosize_key in sorted(iosizes_keys):
      iosize_samples = optype_iosizes[iosize_key]
      sample_list_keys = sorted(iosize_samples.keys())
      if len(sample_list_keys) < max_sample:
        print(
            "WARNING: op-type %s does not have %d samples, has only %d samples"
            % (optype, max_sample, len(sample_list_keys))
        )
      iops_samples = []
      for s in sorted(sample_list_keys):
        sample_pods = iosize_samples[s]
        sample_pods_keys = sample_pods.keys()
        if len(sample_pods_keys) != numjobs * pods_per_run:
            print(
                "WARNING: only %d pods found in optype %s sample %d, expected %d"
                % (len(sample_pods_keys), optype, s, numjobs)
            )
        total_MiB_per_sec = 0.0
        total_iops = 0.0
        elapsed_times = []
        for p in sorted(sample_pods_keys):
                (elapsed, MiB_per_sec, iops) = sample_pods[p]
                total_MiB_per_sec += MiB_per_sec
                total_iops += iops
                print(
                    "%10s, %8d, %2d, %-40s, %f, %f, %f"
                    % (optype, iosize_key, s, p, iops, MiB_per_sec, elapsed)
                )
                elapsed_times.append(elapsed)
        sample_pods["_MiB_per_sec"] = total_MiB_per_sec
        sample_pods["_iops"] = total_iops
        sample_pods["_elapsed_mean"] = mean_elapsed = sum(elapsed_times) / len(elapsed_times)
        sample_pods["_elapsed_pctdev"] = pctdev_elapsed = numpy.std(numpy.array(elapsed_times)) * 100.0 / mean_elapsed
        print(
            "%10s, %8d, %2d, %-40s, %f, %f, %f, %f"
            % (
                optype,
                iosize_key,
                s,
                "all-pods",
                total_iops,
                total_MiB_per_sec,
                mean_elapsed,
                pctdev_elapsed,
            )
        )
        if pctdev_elapsed > elapsed_pctdev_threshold:
            print('WARNING: %% dev. for elapsed time for op %s iosize %d sample %d is %f %%, threshold is %f %%'
                    % (optype, iosize_key, s, pctdev_elapsed, elapsed_pctdev_threshold))
        iops_samples.append(total_iops)
      iops_mean_across_samples = numpy.average(iops_samples)
      iosize_samples['_iops_mean'] = iops_mean_across_samples
      iops_pctdev_across_samples = 100.0 * numpy.std(iops_samples) / iops_mean_across_samples
      iosize_samples['_iops_pctdev'] = iops_pctdev_across_samples
      print("%10s, %8d, mean iops %f, %% dev iops %f" 
              % (optype, iosize_key, iops_mean_across_samples, iops_pctdev_across_samples))
      if len(iops_samples) < 3:
        print(
            "WARNING: only %d samples for op %s, percent deviation cannot be measured"
            % (len(iops_samples), optype)
        )
  optype_dict['_numjobs'] = numjobs
  optype_dict['_max_sample'] = max_sample
  optype_dict['_pods_per_run'] = pods_per_run
  return optype_dict


r1 = analyze_uuid(uuid1)
r2 = analyze_uuid(uuid2)

if r1['_numjobs'] != r2['_numjobs']:
    raise FioStatException('numjobs does not match for uuids %s and %s' % (uuid, uuid2))
if r1['_max_sample'] != r2['_max_sample']:
    raise FioStatException('samples does not match for uuids %s and %s' % (uuid, uuid2))
if r1['_pods_per_run'] != r2['_pods_per_run']:
    raise FioStatException('pods_per_run does not match for uuids %s and %s' % (uuid, uuid2))

incomplete_comparison = False

# this function allows us to try whatever subset of data is overlapping in the two tests
def union(list1, list2):
    return [ el for el in set(list1).union(set(list2)) if isinstance(el, int) or not el.startswith('_') ]

def get_iosize_keys(rslt, rslt_index, op_in):
    try:
        iosize_keys = rslt[op_in].keys()
    except KeyError:
        print('WARNING: op %s was not in result %d' % 
                (op_in, rslt_index))
        incomplete_comparison = True
        iosize_keys = None
    return iosize_keys

def get_sample_keys(rslt, rslt_index, iosize_in, op_in):
    try:
        sample_keys = rslt[iosize_in].keys()
    except KeyError:
        print('WARNING: for op %s, iosize %d was not in result %d' 
                % (op_in, rslt_index))
        incomplete_comparison = True
        sample_keys = None
    return sample_keys

print('')
print('comparing results...')

op_keys_r1 = r1.keys()
op_keys_r2 = r2.keys()
all_ops = union(op_keys_r1, op_keys_r2)
for op in sorted(list(all_ops)):
    #if op.startswith('_'):
    #    continue  # skip keys prefixed with _
    iosize_keys_r1 = get_iosize_keys(r1, 1, op)
    iosize_keys_r2 = get_iosize_keys(r2, 2, op)
    if iosize_keys_r1 is None or iosize_keys_r2 is None:
        continue
    all_iosize_keys = union(iosize_keys_r1, iosize_keys_r2)
    iosize_data_r1 = r1[op]
    iosize_data_r2 = r2[op]
    for iosize in sorted(list(all_iosize_keys)):
        sample_keys_r1 = get_sample_keys(iosize_data_r1, 1, iosize, op)
        sample_keys_r2 = get_sample_keys(iosize_data_r2, 2, iosize, op)
        if sample_keys_r1 is None or sample_keys_r2 is None:
            continue
        all_sample_keys = union(sample_keys_r1, sample_keys_r2)
        sample_data_r1 = iosize_data_r1[iosize]
        sample_data_r2 = iosize_data_r2[iosize]
        samples1 = []
        samples2 = []
        for s in sorted(list(all_sample_keys)):
            #if str(s).startswith('_'):
            #    continue  # skip keys prefixed with _
            try:
                samples1.append(sample_data_r1[s]['_iops'])
            except KeyError:
                print('test 1 op %s iosize %d sample %d missing' % (op, iosize, s))
            try:
                samples2.append(sample_data_r2[s]['_iops'])
            except KeyError:
                print('test 2 op %s iosize %d sample %d missing' % (op, iosize, s))

        # do not compare sample sets unless we have enough of them

        if len(samples1) < 3:
            print('WARNING: test 1 op %s iosize %d has only %d samples!' 
                    %(op, iosize, len(samples1)))
            continue
        if len(samples2) < 3:
            print('WARNING: test 2 op %s iosize %d has only %d samples!' 
                    %(op, iosize, len(samples2)))
            continue

        # do statistical test to see if these two sets of results are truly different
        (t, prob) = mannwhitneyu(samples1, samples2)
        
        if (1.0 - prob) > confidence_threshold:
            mean1 = sample_data_r1['_iops_mean']
            mean2 = sample_data_r2['_iops_mean']
            mean_diff_pct = 100.0 * (mean2 - mean1) / mean1
            if mean_diff_pct > mean_difference_pct_threshold:
                print('WARNING: for op %s iosize %d, result 1 mean %f and result 2 mean %f are distinguishable with probability %f'
                    %(op, iosize, mean1, mean2, (1.0 - prob)))

