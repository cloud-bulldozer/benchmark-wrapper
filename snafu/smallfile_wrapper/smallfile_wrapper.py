#!/usr/bin/env python
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
import logging
import os
import sys
import numpy
from snafu.utils.fetch_es_test_results import result_generator_for_uuid

from . import trigger_smallfile

KiB_per_MiB = 1 << 10

logger = logging.getLogger("snafu")


class SnafuStorageException(Exception):
    pass


class smallfile_wrapper:
    def __init__(self, parser):

        # run_snafu.py should call down to self.post_process()
        self.requires_postprocessing = True

        # collect arguments

        # it is assumed that the parser was created using argparse and already knows
        # about the --tool option
        parser.add_argument(
            "-s", "--samples", type=int, help="number of times to run benchmark, defaults to 1", default=1
        )
        parser.add_argument("-T", "--top", help="directory to access files in")
        parser.add_argument(
            "-d", "--dir", help="output parent directory", default=os.path.dirname(os.getcwd())
        )
        parser.add_argument(
            "-o", "--operations", help="sequence of smallfile operation types to run", default="create"
        )
        parser.add_argument("-y", "--yaml-input-file", help="smallfile parameters passed via YAML input file")
        self.args = parser.parse_args()

        self.server = ""

        self.cluster_name = "mycluster"
        if "clustername" in os.environ:
            self.cluster_name = os.environ["clustername"]

        self.uuid = ""
        if "uuid" in os.environ:
            self.uuid = os.environ["uuid"]

        self.user = ""
        if "test_user" in os.environ:
            self.user = os.environ["test_user"]

        self.redis_host = os.environ["redis_host"] if "redis_host" in os.environ else None
        self.redis_timeout = os.environ["redis_timeout"] if "redis_timeout" in os.environ else 60
        self.redis_timeout_th = os.environ["redis_timeout_th"] if "redis_timeout_th" in os.environ else 200
        self.clients = os.environ["clients"] if "clients" in os.environ else 1
        if not self.args.top:
            raise SnafuStorageException("must supply directory where you access files")
        self.samples = self.args.samples
        self.working_dir = self.args.top
        self.result_dir = self.args.dir
        self.yaml_input_file = self.args.yaml_input_file
        self.operations = self.args.operations

    def run(self):
        if not os.path.exists(self.result_dir):
            os.mkdir(self.result_dir)
        for s in range(1, self.samples + 1):
            sample_dir = self.result_dir + "/" + str(s)
            if not os.path.exists(sample_dir):
                os.mkdir(sample_dir)
            for o in self.operations.split(","):
                trigger_generator = trigger_smallfile._trigger_smallfile(
                    logger,
                    o,
                    self.yaml_input_file,
                    self.cluster_name,
                    self.working_dir,
                    sample_dir,
                    self.user,
                    self.uuid,
                    self.redis_host,
                    self.redis_timeout,
                    self.redis_timeout_th,
                    self.clients,
                    s,
                )
                yield trigger_generator

    # calculate statistics from raw elasticsearch results
    # generated by run(self): above

    def post_process(self):
        KiB_per_MiB = 1 << 10

        index_name = "ripsaw-smallfile-results"
        # used by strptime()
        datetime_format = "%Y-%m-%dT%H:%M:%S.%f%z"

        uuid = self.uuid
        uuid_query = {
            "query": {"simple_query_string": {"query": uuid, "fields": ["uuid"], "default_operator": "and"}}
        }
        es = self.index_args.elasticsearch_obj
        optype_dict = {}
        result_generator = result_generator_for_uuid(es, self.index_name, uuid)
        for hit in result_generator:
            src = hit["_source"]
            uuid_found = src["uuid"]
        
            logger.debug(json.dumps(src, indent=2))
            sample = int(src["sample"])
            files = int(src["files"])
            records = int(src["records"])
            file_size_KiB = int(src["params"]["file-size"])
            files_per_sec = float(src["files-per-sec"])
            MiB_per_sec = files_per_sec * file_size_KiB / KiB_per_MiB
            elapsed = float(src["elapsed"])
            iops = float(records) / elapsed
            optype = src["optype"]
            thrd_id = int(src["tid"])
            # host is really pod name
            host = src["host"]
            if not threads_per_pod:
                threads_per_pod = int(src["params"]["threads"])
        
            # build up a tree optype -> sample -> pod -> thread
            # so we can compute stats

            # optype layer
            try:
                optype_samples = optype_dict[optype]
            except KeyError:
                optype_samples = {}
                optype_dict[optype] = optype_samples

            if max_sample < sample:
                max_sample = sample

            # sample layer
            try:
                sample_pods = optype_samples[sample]
            except KeyError:
                sample_pods = {}
                optype_samples[sample] = sample_pods

            # pod layer
            try:
                sample_threads = sample_pods[host]
            except KeyError:
                sample_threads = {}
                sample_pods[host] = sample_threads

            # thread layer (can be multiple threads per pod)
            try:
                any_prev_tupl = sample_threads[thrd_id]
                raise SMFStatException("2 samples with same optype and thread ID should not happen: %s" % str(src))
            except KeyError:
                # this is the NORMAL case
                tupl = (elapsed, files, files_per_sec, MiB_per_sec, iops)
                sample_threads[thrd_id] = tupl

        # now validate data
        
        logger.debug("optypes in test: %s" % str(optype_dict.keys()))
        result_list = []
        for optype in sorted(optype_dict.keys()):
            optype_samples = optype_dict[optype]
            sample_list_keys = sorted(optype_samples.keys())
            if len(sample_list_keys) < max_sample:
                logger.warning(
                    "WARNING: op-type %s does not have %d samples, has only %d samples"
                    % (optype, max_sample, len(sample_list_keys))
                )
            fps_samples = []
            for s in sorted(sample_list_keys):
                sample_pods = optype_samples[s]
                sample_pods_keys = sample_pods.keys()
                if pods_per_run == 0:
                    pods_per_run = len(sample_pods_keys)
                elif len(sample_pods_keys) < pods_per_run:
                    logger.warning(
                        "WARNING: only %d pods found in optype %s sample %d, expected %d"
                        % (len(sample_pods_keys), optype, s, pods_per_run)
                    )
                total_files = 0
                total_files_per_sec = 0.0
                total_MiB_per_sec = 0.0
                total_iops = 0.0
                elapsed_times = []
                for p in sorted(sample_pods_keys):
                    sample_threads = sample_pods[p]
                    sample_threads_keys = sample_threads.keys()
                    if len(sample_threads_keys) != threads_per_pod:
                        logger.warning(
                            "WARNING: only %d threads found, expected %d"
                            % (len(sample_threads_keys), threads_per_pod)
                        )
                    for t in sorted(sample_threads_keys):
                        (elapsed, files, files_per_sec, MB_per_sec, iops) = sample_threads[t]
                        total_MiB_per_sec += MiB_per_sec
                        total_files_per_sec += files_per_sec
                        total_iops += iops
                        total_files += files
                        logger.debug(
                            "%10s, %2d, %-40s, %3s, %d, %f, %f, %f"
                            % (optype, s, p, t, files, files_per_sec, MB_per_sec, elapsed)
                        )
                        elapsed_times.append(elapsed)
                sample_pods["_MiB_per_sec"] = total_MiB_per_sec
                sample_pods["_files_per_sec"] = total_files_per_sec
                sample_pods["_iops"] = total_iops
                sample_pods["_elapsed_mean"] = mean_elapsed = sum(elapsed_times) / len(elapsed_times)
                sample_pods["_elapsed_pctdev"] = pctdev = numpy.std(numpy.array(elapsed_times)) * 100.0 / mean_elapsed
                logger.debug(
                    "%10s, %2d, %-40s, %3s, %d, %f, %f, %f, %f, %f"
                    % (
                        optype,
                        s,
                        "all-pods",
                        "all-threads",
                        total_files,
                        total_files_per_sec,
                        total_MiB_per_sec,
                        total_iops,
                        mean_elapsed,
                        pctdev,
                    )
                )
                fps_samples.append(total_files_per_sec)
            fps_mean_across_samples = numpy.average(fps_samples)
            fps_pctdev_across_samples = 100.0 * numpy.std(fps_samples) / fps_mean_across_samples
            r = {}
            r['uuid'] = uuid
            r['optype'] = optype
            r['mean'] = fps_mean_across_samples
            r['pctdev'] = fps_pctdev_across_samples
            logger.debug("%10s, mean fps %f, %% dev fps %f" % (optype, fps_mean_across_samples, fps_pctdev_across_samples))
            if len(fps_samples) < 3:
                logger.warning(
                    "WARNING: only %d samples for op %s, percent deviation cannot be measured"
                    % (len(fps_samples), optype)
                )
            result_list.append(r)
        return result_list
